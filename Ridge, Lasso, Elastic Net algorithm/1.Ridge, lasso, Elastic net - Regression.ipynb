{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Even without any knowledge of machine learning, you can say that if you have to predict sales for an item – it would be \n",
    "the average over last few days . / months / weeks.                                                                             \n",
    "Turns out that there are various ways in which we can evaluate how good is our model.                                          \n",
    "The most common way is Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To evaluate how good is a model, lets understand the impact of wrong predictions. If we predict sales to be higher than \n",
    "what they might be, the store will spend a lot of money making unnecessary arrangement which would lead to excess inventory.\n",
    "On the other side if I predict it too low, I will lose out on sales opportunity.\n",
    "\n",
    "1.Mean Squared error :                                                                                                         \n",
    "    to calculate the difference in the predicted and actual values. However, if we simply add them, they might cancel out,\n",
    "    so we square these errors before adding. We also divide them by the number of data points to calculate a mean square error, \n",
    "    since it should not be dependent on number of data points.\n",
    "                     \n",
    "                ( e1^2 + e2^2 + ..... + en^2 ) / n           # e1, e2 : difference between the actual and the predicted values,\n",
    "                                                             # n - no. of data points       \n",
    "                                                             \n",
    "                mse = np.mean((y_pred - y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.The Best Line Fit :   (of Linear regression)                                                                                 \n",
    "    The main purpose of the best fit line is that our predicted values should be closer to our actual or the observed values,\n",
    "    because there is no point in predicting values which are far away from the real values. In other words, we tend to minimize\n",
    "    the difference between the values predicted by us and the observed values, and which is actually termed as error.\n",
    "    These errors are also called as residuals. The residuals are indicated by the vertical lines showing the difference \n",
    "    between the predicted and actual value.\n",
    "   \n",
    "    Sum of Square of Residuals :   \n",
    "        it’s the method mostly used in practice, here we penalize higher error value much more as compared to smaller one,\n",
    "        so that there is a significant difference between making big errors and small errors, which makes it easy \n",
    "        to differentiate and select the best fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='sum_of_square_residuals.png'>\n",
    "\n",
    "h(x) is the value predicted by us( h(x) =Θ1*x +Θ0 ) , y is the actual values and m is the number of rows in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.The Cost function :                                                                                                          \n",
    "    So let’s say, you increased the size of a particular shop, where you predicted that the sales would be higher.\n",
    "    But despite increasing the size, the sales in that shop did not increase that much. So the cost applied in increasing\n",
    "    the size of the shop, gave you negative results. So, we need to minimize these costs.\n",
    "    Therefore we introduce a cost function, which is basically used to define and measure the error of the model.\n",
    "         \n",
    "--> it is just similar to sum of squared errors, with just a factor of 1/2m is multiplied in order to ease mathematics.\n",
    "    to improve our prediction, we need to minimize the cost function. For this purpose we use the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='cost_function.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.Gradient Descent :                                                                                                          \n",
    "    Gradient Descent -- is an algorithm that finds best fit line for given training data set. (MSE=Cost function are similar)\n",
    "    Y= 5x + 4x^2.                                                                                                              \n",
    "    In mathematics, we simple take the derivative of this equation with respect to x, simply equate it to zero.\n",
    "    This gives us the point where this equation is minimum. Therefore substituting that value can give us the minimum value of\n",
    "    that equation. Gradient descent works in a similar manner. It iteratively updates Θ, to find a point where the cost function\n",
    "    would be minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. R-Square  and  Adjusted R-Square :                                                                                           \n",
    "\n",
    "How accurate do you think the model is? Actually we have a quantity, known as R-Square.  (or Its an accuracy check)\n",
    "    \n",
    "R-Square:                                                                                                                      \n",
    "        It determines how much of the total variation in Y (dependent variable) is explained by the variation in\n",
    "        X (independent variable). \n",
    "        \n",
    "        The value of R-square is always between 0 and 1, where 0 means that the model does not explain any variability \n",
    "        in the target variable (Y)  and  1 means it explains full variability in the target variable(Y).\n",
    "    \n",
    "    The drawback of R-Square is that if new predictors (X) are added to our model, R-Square only increases or remains constant\n",
    "    but it never decreases. We can not judge that by increasing complexity of our model, are we making it more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='R-square.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adjusted R-Square :                                                                                                            \n",
    "        is the modified form of R-Square that has been adjusted for the number of predictors in the model. It incorporates\n",
    "        model’s degree of freedom. The adjusted R-Square only increases if the new term improves the model accuracy.\n",
    "                        \n",
    "        R^2 = Sample R square , p = Number of predictors , N = total sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='adjusted-r-square.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.Overfitting (High variance) and  Underfitting (High bias)\n",
    "\n",
    "To overcome underfitting(high bias), we basically add new parameters to our model so that the model complexity increases,\n",
    "and thus reducing high bias.\n",
    "\n",
    "how can we overcome Overfitting for a regression model :  there are two methods to overcome overfitting\n",
    "    ->Reduce the model complexity (decrease the no. of parameters to model)\n",
    "    ->Regularization\n",
    "    \n",
    "In regularization, what we do is normally we keep the same number of features, but reduce the magnitude of the coefficients.    \n",
    "For this purpose, we have different types of regression techniques which uses regularization to overcome this problem.\n",
    "\n",
    "-->Ridge regression  \n",
    "(It uses L2 regularization technique)\n",
    "\n",
    "-->Lasso regression \n",
    "(It uses L1 regularization technique)\n",
    "\n",
    "-->Elastic Net regression\n",
    "Elastic Net regression : generally works well when we have a big dataset. It is combination of both L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Linear Regression for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility',\n",
       "       'Item_Type', 'Item_MRP', 'Outlet_Identifier',\n",
       "       'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type',\n",
       "       'Outlet_Type', 'Item_Outlet_Sales'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.loc[:, ['Outlet_Establishment_Year','Item_MRP']]\n",
    "y = train.Item_Outlet_Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(X, y)         # cv - for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LinearRegression()        # Linear Regression\n",
    "\n",
    "lreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lreg.predict(X_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Mean Squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1988422.3439036114"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = np.mean((pred - y_cv)**2)    # pred : predicted value , y_cv : actual value\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.8613268, 15.3293233])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.coef_         # gives coefficient of both columns : 'Outlet_establishment_year'  and  'Item_MRP'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17711.056924832614"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3437781582125675"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.score(X_cv, y_cv)         # Its check the accuracy of model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what would happen if I introduce one more feature in my model, will my model predict values more closely to its actual\n",
    "value? Will the value of R-Square increase? Let us consider another case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with more variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Item_Weight'].fillna((train['Item_Weight'].mean()), inplace=True)    # 'Item_Weight' contains NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.loc[:,['Outlet_Establishment_Year','Item_MRP','Item_Weight']]         # 'Item_Weight'  also taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_cv, y_train, y_cv = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lreg.predict(x_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2031305.8585451297"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = np.mean((pred - y_cv)**2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error :  2031305.8585451297\n",
      "Coefficients are :  [-11.17720757  15.43323197  -2.37218252]\n",
      "R-Square Score :  0.3319108998142322\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error : ', mse)\n",
    "print('Coefficients are : ', lreg.coef_)\n",
    "print('R-Square Score : ', lreg.score(x_cv, y_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all the features for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility',\n",
       "       'Item_Type', 'Item_MRP', 'Outlet_Identifier',\n",
       "       'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type',\n",
       "       'Outlet_Type', 'Item_Outlet_Sales'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Item_Visibility'] = train['Item_Visibility'].replace(0, np.mean(train['Item_Visibility']))\n",
    "train['Outlet_Establishment_Year'] = 2013 - train['Outlet_Establishment_Year']\n",
    "train['Outlet_Size'].fillna('Small', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy var. for categorical data\n",
    "\n",
    "mylist = list(train.select_dtypes(include=['object']).columns)\n",
    "dummies = pd.get_dummies(train[mylist], prefix=mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(mylist, axis=1, inplace=True)\n",
    "\n",
    "X = pd.concat([train, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Item_Outlet_Sales', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_cv, y_train, y_cv = train_test_split(X, train.Item_Outlet_Sales, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cv = lreg.predict(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((pred_cv - y_cv)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error :  1927718.664115257\n",
      "R-Square :  0.3357450860849427\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error : ', mse)\n",
    "print('R-Square : ', lreg.score(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Residual plot')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_plot = plt.scatter(pred_cv, (pred_cv - y_cv), c='b')\n",
    "\n",
    "plt.hlines(y=0, xmin= -1000, xmax=5000)\n",
    "\n",
    "plt.title('Residual plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from pandas import Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeReg = Ridge(alpha=0.05, normalize=True)\n",
    "\n",
    "ridgeReg.fit(X_train,y_train)\n",
    "\n",
    "pred = ridgeReg.predict(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((pred_cv - y_cv)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1927718.664115257"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error :  1927718.664115257\n",
      "R-Square :  0.33597028421136554\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error : ', mse)\n",
    "print('R-Square : ', ridgeReg.score(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets consider different values of alpha(alpha=0.05,0.5,5,10 ) and plot the coefficients.                                     \n",
    "As increases the value of alpha, 'magnitude of coefficients decreases' where the values reaches to zero but not absolute zero.\n",
    "calculate R-square for each alpha, using the one which gives us lowest error.                                                  \n",
    "\n",
    "If you notice, we come across an extra term, which is known as the 'penalty' term.                                            \n",
    "'λ' given here, is actually denoted by alpha parameter in the 'ridge function'. So by changing the values of alpha, we are\n",
    "basically controlling the penalty term. Higher the values of alpha, bigger is the penalty and therefore the magnitude of \n",
    "coefficients are reduced.\n",
    "\n",
    "It shrinks the parameters, therefore it is mostly used to prevent multicollinearity.                                          \n",
    "It reduces the model complexity by coefficient shrinkage.                                                                      \n",
    "It uses L2 regularization technique.                                                                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression  (Least Absolute Shrinkage Selector Operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lassoReg = Lasso(alpha=0.3, normalize=True)   # aplpha=0.05,0.5\n",
    "\n",
    "lassoReg.fit(X_train,y_train)\n",
    "\n",
    "pred = lassoReg.predict(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = np.mean((pred_cv - y_cv)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error :  1927718.664115257\n",
      "R-Square :  0.3358128986448432\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error : ', mse)\n",
    "print('R-Square : ', lassoReg.score(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lets change the value of alpha and see how does it affect the coefficients.                                                    \n",
    "we can see that even at small values of alpha, the magnitude of coefficients have reduced a lot.                               \n",
    "\n",
    "In case of Ridge as we increased the value of alpha, coefficients were approaching towards zero, \n",
    "But if you see in case of 'lasso' , even at smaller alpha’s, our coefficients are reducing to absolute zeroes.                \n",
    "Therefore, lasso selects the only some feature while reduces the coefficients of others to zero.                            \n",
    "This property is known as feature selection and which is absent in case of ridge.                                           \n",
    "\n",
    "It uses L1 regularization technique.                                                                                          \n",
    "It is generally used when we have more number of features, because it automatically does feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "ENreg = ElasticNet(alpha=1, l1_ratio=0.5, normalize=False)\n",
    "\n",
    "ENreg.fit(X_train,y_train)\n",
    "\n",
    "pred_cv = ENreg.predict(X_cv)\n",
    "\n",
    "\n",
    "mse = np.mean((pred_cv - y_cv)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error :  1973094.6831021125\n",
      "R-Square :  0.3358128986448432\n"
     ]
    }
   ],
   "source": [
    "print('Mean Squared Error : ', mse)\n",
    "print('R-Square : ', ENreg.score(X_cv, y_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic net regression generally works well when we have a big dataset.                                                     \n",
    "\n",
    "Elastic net is basically a combination of both L1 and L2 regularization. So if you know elastic net, you can implement both\n",
    "'Ridge' and 'Lasso' by tuning the parameters. So it uses both L1 and L2 penality term.                                      \n",
    "\n",
    "Let’ say, we have a bunch of correlated independent variables in a dataset, then elastic net will simply form a group consists\n",
    "of these correlated variables. Now if any one of the variable of this group is a strong predictor\n",
    "(means : having a strong relationship with dependent variable), then we will include the entire group in the model building,\n",
    "because omitting other variables (like what we did in lasso) might result in losing some information in terms of interpretation\n",
    "ability, leading to a poor model performance.\n",
    "\n",
    "                alpha = a + b  and  l1_ratio =  a / (a+b)\n",
    "\n",
    "'a' and 'b' weights assigned to L1 and L2. So when we change the values of alpha and l1_ratio,                                 \n",
    "'a' and 'b' are set accordingly such that they control trade off between L1 and L2.                                            \n",
    "\n",
    "\n",
    "                a * (L1 term) + b* (L2 term)\n",
    "    \n",
    "a * (L1 term) + b* (L2 term) , consider the following cases :                                                                  \n",
    "    -If l1_ratio =1, l1_ratio can only be equal to 1 if a=1, which implies b=0. Therefore, it will be a lasso penalty.         \n",
    "    -Similarly if l1_ratio = 0, implies a=0. Then the penalty will be a ridge penalty.                                         \n",
    "    -For l1_ratio between 0 and 1, the penalty is the combination of ridge and lasso.                                          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
