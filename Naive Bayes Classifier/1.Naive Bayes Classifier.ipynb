{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive Bayes algorithms are mostly used in sentiment analysis, spam filtering, recommendation systems etc.                       \n",
    "They are fast and easy to implement but their biggest disadvantage is that the requirement of predictors to be independent.     \n",
    "In most of the real life cases, the predictors are dependent, this hinders the performance of the classifier.                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Multinomial Naive Bayes:                                                                                                       \n",
    "This is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics,\n",
    "technology etc. The features/predictors used by the classifier are the frequency of the words present in the document.\n",
    "            \n",
    "Bernoulli Naive Bayes :                                                                                                        \n",
    "This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict\n",
    "the class variable take up only values yes or no, for example if a word occurs in the text or not.\n",
    "            \n",
    "Gaussian Naive Bayes :                                                                                                         \n",
    "When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes Theorem : P(A|B) = P(B|A).P(A) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are taking the SMS dataset, based on the text predict whether the message is 'Spam' or 'ham' using NLP also. \n",
    "#### dataset downladed from kaggle : https://www.kaggle.com/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.Importing the Dataset\n",
    "\n",
    "import pandas as pd\n",
    "messages = pd.read_csv('SMSSpamCollection.csv', sep='\\t', names=['label', 'message'])\n",
    "# Now, spam and message is separated with tab-space here we use delimiter '\\t' - indicates tab \n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Data Cleaning and preprocessing  (In message- the, (,) (.) stopwords are there those are not useful to tell its spam or not)\n",
    "\n",
    "import re \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer          # also can use : lemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "corpus = []   # after preprocessing messages we put into corpus.\n",
    "\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])      # remove punctuations using 're' - regular expression\n",
    "    review = review.lower()                                        # lower all the words \n",
    "    review = review.split()                                        # remove spaces\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if word not in stopwords.words('english')]   # remove stopwords\n",
    "    review = ' '.join(review)                                                               # join the words into list\n",
    "    corpus.append(review)                                                                   # finally append into 'corpus' list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Create Bag of Words (Document matrix) \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer       # also can use : TF-IDF vectorizer\n",
    "\n",
    "cv =CountVectorizer(max_features=5000)  # max-features using top 5000 words, instead of using all in data, change acc. \n",
    "X = cv.fit_transform(corpus).toarray()  # x - is independent feature, label -in message is dependent feature (y-becomes label)\n",
    "\n",
    "y= pd.get_dummies(messages['label'])   # create 2 column of dummies, using for label(ham, spam) \n",
    "y = y.iloc[:,1].values  # it removes 1 column, not need of 2, we use only 1(indicates 0: ham, 1: spam), Now y -dependent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Train-Test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Training model - Naives Bayes Classifier (using bcoz NaiveBayes works well w.r.t NLP , also works on probability)\n",
    "\n",
    "from sklearn.naive_bayes import  GaussianNB, MultinomialNB, BernoulliNB      # Multinomail works for any no. of classes\n",
    "\n",
    "spam_detect_model = MultinomialNB().fit(X_train,y_train)  # 1st train model \n",
    "\n",
    "y_pred = spam_detect_model.predict(X_test)      # then we predict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix : \n",
      " [[946   9]\n",
      " [  8 152]]\n",
      "\n",
      "Accuracy is : 98.48%\n"
     ]
    }
   ],
   "source": [
    "# To compare y_pred and y_test \n",
    "# using - confusion_matrix (it gives 2x2 matrix, which says how many elements are correctly predicted)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)    # In matrix, vertical(shows actual output) , horizontal(predicted output)\n",
    "print('Confusion matrix : \\n', cm)\n",
    "\n",
    "# To check accuracy\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f'\\nAccuracy is : {accuracy:0.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
